{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d2f2144",
   "metadata": {
    "papermill": {
     "duration": 0.012534,
     "end_time": "2024-11-16T21:12:49.038327",
     "exception": false,
     "start_time": "2024-11-16T21:12:49.025793",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Pix2Pix Image Translation: SAR to RGB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bed901",
   "metadata": {
    "papermill": {
     "duration": 0.011303,
     "end_time": "2024-11-16T21:12:49.062471",
     "exception": false,
     "start_time": "2024-11-16T21:12:49.051168",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This is a Pix2Pix CGAN implementation for translating Synthetic Aperture Radar (SAR) images to optical images. Throughout the notebook, we first implement the pix2pix architecture from building blocks to the whole architecture step by step. If you are not familiar with the architecture or want to brush up on the concepts, you can consider reading these papers:\n",
    "1. Generative Adversarial Networks (GANs): [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661), (Goodfellow, et al. 2014)\n",
    "2. U-net: [U-net: Convolutional networks for biomedical image segmentation](https://arxiv.org/abs/1505.04597), (Ronneberger, et al. 2015)\n",
    "3. Pix2Pix: [Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/abs/1611.07004), (Isola, et al. 2017)\n",
    "\n",
    "The Pix2Pix model is an architecture designed for image-to-image translation tasks, such as converting sketches to photos or colorizing black-and-white images. The Pix2Pix architecture uses two main components:\n",
    "\n",
    "1. Generator: The generator network creates fake images from input data, using an U-Net like architecture.\n",
    "\n",
    "2. Discriminator: The discriminator distinguishes between real and fake images. The goal of the discriminator is to classify whether a given image is real or fake. In Pix2Pix, a 70x70 PatchGAN is often used, where the discriminator processes image patches rather than the full image.\n",
    "\n",
    "The model is trained using adversarial loss and a reconstruction loss (L1 loss) to encourage the generator to produce high-quality images that are close to the ground truth.\n",
    "\n",
    "I will use PyTorch for implementing the model and the Sentinel SAR & Optical Image Pairs dataset. The dataset is available on [Kaggle](https://www.kaggle.com/) at [Sentinel-1&2 Image Pairs (SAR & Optical)](https://www.kaggle.com/datasets/requiemonk/sentinel12-image-pairs-segregated-by-terrain).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf714d95",
   "metadata": {
    "papermill": {
     "duration": 0.011326,
     "end_time": "2024-11-16T21:12:49.085536",
     "exception": false,
     "start_time": "2024-11-16T21:12:49.074210",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "378bf6a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T21:12:49.110067Z",
     "iopub.status.busy": "2024-11-16T21:12:49.109675Z",
     "iopub.status.idle": "2024-11-16T21:12:53.755497Z",
     "shell.execute_reply": "2024-11-16T21:12:53.754684Z"
    },
    "papermill": {
     "duration": 4.660841,
     "end_time": "2024-11-16T21:12:53.757881",
     "exception": false,
     "start_time": "2024-11-16T21:12:49.097040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from enum import Enum\n",
    "from typing import Tuple, List, Optional, Callable, Union, Literal\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f30b53",
   "metadata": {
    "papermill": {
     "duration": 0.011427,
     "end_time": "2024-11-16T21:12:53.782324",
     "exception": false,
     "start_time": "2024-11-16T21:12:53.770897",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prepare The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafd94d8",
   "metadata": {
    "papermill": {
     "duration": 0.012384,
     "end_time": "2024-11-16T21:12:53.806109",
     "exception": false,
     "start_time": "2024-11-16T21:12:53.793725",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here is a custom PyTorch dataset class. We will use it later when we train our model. This class helps us manage the dataset conveniently. Technically, it's not always necessary to write your own dataset classes, but I prefer to do so because it provides more flexibility and control. Writing a custom dataset class is quite simple: Your class should inherit from the ``Dataset`` class and override the following two methods:\n",
    "    1. `__len__`: This method returns the size of the dataset when you call `len(dataset)`\n",
    "    2. `__getitem__`: his method allows indexing, so `dataset[i]` will return the i-th example.\n",
    "\n",
    "In our implementation, we’ve added extra methods to assist with finding pairs, splitting the data, and creating and saving data splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4f775ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T21:12:53.836469Z",
     "iopub.status.busy": "2024-11-16T21:12:53.835487Z",
     "iopub.status.idle": "2024-11-16T21:12:53.881598Z",
     "shell.execute_reply": "2024-11-16T21:12:53.880489Z"
    },
    "papermill": {
     "duration": 0.064497,
     "end_time": "2024-11-16T21:12:53.884359",
     "exception": false,
     "start_time": "2024-11-16T21:12:53.819862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SplitType(Enum):\n",
    "    \"\"\"Enumeration for dataset split types\"\"\"\n",
    "    TRAIN = 'train'\n",
    "    VAL = 'val'\n",
    "    TEST = 'test'\n",
    "\n",
    "\n",
    "class Sentinel(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for handling Sentinel-1&2 Image Pairs.\n",
    "    \n",
    "    This dataset assumes a directory structure of:\n",
    "    root_dir/\n",
    "        category1/\n",
    "            s1/\n",
    "                image1.png\n",
    "                image2.png\n",
    "            s2/\n",
    "                image1.png\n",
    "                image2.png\n",
    "        category2/\n",
    "            ...\n",
    "    \n",
    "    This class has support for train/val/test splits. When `split_type` is `None`, \n",
    "    uses the complete dataset. When `split_type` is specified \n",
    "    (``'train'``, ``'val'``, ``'test'``), the dataset can be split using:\n",
    "\n",
    "    1. A split that defines which images belong to which split\n",
    "    2. Random splitting with a specified ratio\n",
    "\n",
    "    Args:\n",
    "        root_dir (str | Path): Root directory containing the dataset\n",
    "        split_type (str | None): Which split to use ('train', 'val', 'test') or None for full dataset\n",
    "        transform (callable, optional): Transform to apply to both SAR and optical images\n",
    "        split_mode (str, optional): How to split the dataset ('random', 'split')\n",
    "        split_ratio (Tuple[float, float, float], optional): Ratio for train/val/test splits\n",
    "        split_file (str | Path, optional): predefined the splits\n",
    "        seed (int, optional): Random seed for reproducible splitting\n",
    "        \n",
    "    Attributes:\n",
    "        root_dir (Path): Path to the dataset root directory\n",
    "        transform (callable): Transform pipeline for the images\n",
    "        image_pairs (List[Tuple[Path, Path]]): List of paired image paths (SAR, optical)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 root_dir: Union[str, Path],\n",
    "                 split_type: Optional[str] = None,\n",
    "                 transform: Optional[Callable] = None,\n",
    "                 split_mode: Literal['random', 'split'] = 'random',\n",
    "                 split_ratio: Tuple[float, float, float] = (0.7, 0.15, 0.15),\n",
    "                 split_file: Optional[Union[str, Path]] = None,\n",
    "                 seed: int = 42):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        if not self.root_dir.exists():\n",
    "            raise FileNotFoundError(f\"Dataset root directory not found: {self.root_dir}\")\n",
    "        \n",
    "        # Convert string split_type to enum if provided\n",
    "        self.split_type = SplitType(split_type) if split_type else None\n",
    "\n",
    "        # Default transform pipeline\n",
    "        self.transform = transform if transform else v2.Compose([\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float32, scale=True)\n",
    "        ])\n",
    "\n",
    "        # Collect image pairs\n",
    "        self.all_image_pairs = self._collect_images()\n",
    "\n",
    "        # Apply split if specified\n",
    "        if split_type:\n",
    "            if split_mode == 'split' and split_file:\n",
    "                self.image_pairs = self._apply_predefined_split(split_file)\n",
    "            elif split_mode == 'random':\n",
    "                self.image_pairs = self._apply_random_split(split_ratio, seed)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid split configuration. Use either 'split' with a split_file or 'random' with split_ratio\")\n",
    "        else:\n",
    "            # If no split type specified, use all images\n",
    "            self.image_pairs = self.all_image_pairs\n",
    "\n",
    "        print(f'Total image pairs found: {len(self)}')\n",
    "\n",
    "    def _collect_images(self) -> List[Tuple[Path, Path]]:\n",
    "        \"\"\"\n",
    "        Collects paired SAR (s1) and optical (s2) image paths from the dataset directory.\n",
    "            \n",
    "        Returns:\n",
    "            List[Tuple[Path, Path]]: List of (SAR image path, optical image path) pairs\n",
    "        \"\"\"\n",
    "        image_pairs = []\n",
    "        \n",
    "        # Iterate through category subdirectories\n",
    "        for category in self.root_dir.iterdir():\n",
    "            # Check if it's a directory\n",
    "            if not category.is_dir():\n",
    "                continue\n",
    "\n",
    "            s1_path = category / 's1'\n",
    "            s2_path = category / 's2'\n",
    "            \n",
    "            if not (s1_path.is_dir() and s2_path.is_dir()):\n",
    "                # print(f\"Missing s1 or s2 subdirectory in category: {category.name}\")\n",
    "                continue\n",
    "\n",
    "            # Collect pairs\n",
    "            for s1_file in s1_path.glob('*.png'):\n",
    "                # Convert SAR filename to optical filename\n",
    "                # e.g. 'ROIs1970_fall_s1_13_p265.png' -> 'ROIs1970_fall_s2_13_p265.png'\n",
    "                s2_filename = list(s1_file.name.split('_'))\n",
    "                s2_filename[2] = 's2'\n",
    "                s2_file = s2_path / '_'.join(s2_filename)\n",
    "\n",
    "                if not s2_file.exists():\n",
    "                    # print(f\"Missing optical image for SAR image: {s1_file.name} - {s2_file.name}\")\n",
    "                    continue\n",
    "\n",
    "                image_pairs.append((s1_file, s2_file))\n",
    "        \n",
    "        return image_pairs\n",
    "    \n",
    "    def _apply_predefined_split(self, split_file: Union[str, Path]) -> List[Tuple[Path, Path]]:\n",
    "        \"\"\"\n",
    "        Applies a predefined split from a JSON file.\n",
    "        \n",
    "        Args:\n",
    "            split_file: Path to JSON file containing split definitions\n",
    "            \n",
    "        Returns:\n",
    "            List[Tuple[Path, Path]]: Image pairs for the specified split\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(split_file, 'r') as f: # get the split content\n",
    "                splits = json.load(f)\n",
    "                \n",
    "            if self.split_type.value not in splits['data']: # check if it helds\n",
    "                raise ValueError(f\"Split type {self.split_type.value} not found in split file\")\n",
    "            \n",
    "            split_filenames = set(splits['data'][self.split_type.value]) # data['split']\n",
    "            return [pair for pair in self.all_image_pairs # collect and return split\n",
    "                if any(str(p.relative_to(self.root_dir)) in split_filenames for p in pair[:2])]\n",
    "        except Exception as e:\n",
    "            print(f'Could not open split file\\n\\t{e}')\n",
    "            raise\n",
    "\n",
    "    def _apply_random_split(\n",
    "        self, \n",
    "        split_ratio: Tuple[float, float, float],\n",
    "        seed: int\n",
    "    ) -> List[Tuple[Path, Path]]:\n",
    "        \"\"\"\n",
    "        Randomly splits the dataset according to the given ratios.\n",
    "        \n",
    "        Args:\n",
    "            split_ratio: Tuple of (train, val, test) ratios\n",
    "            seed: Random seed for reproducibility\n",
    "            \n",
    "        Returns:\n",
    "            List[Tuple[Path, Path]]: Image pairs for the specified split\n",
    "        \"\"\"\n",
    "        if sum(split_ratio) != 1:\n",
    "            raise ValueError(\"Split ratios must sum to 1\")\n",
    "        \n",
    "        # Set random seed for reproducibility\n",
    "        random.seed(seed)\n",
    "        \n",
    "        # Shuffle indices\n",
    "        indices = list(range(len(self.all_image_pairs)))\n",
    "        random.shuffle(indices)\n",
    "        \n",
    "        # Calculate split points\n",
    "        train_end = int(len(indices) * split_ratio[0])\n",
    "        val_end = train_end + int(len(indices) * split_ratio[1])\n",
    "        \n",
    "        # Select appropriate slice based on split type\n",
    "        if self.split_type == SplitType.TRAIN:\n",
    "            split_indices = indices[:train_end]\n",
    "        elif self.split_type == SplitType.VAL:\n",
    "            split_indices = indices[train_end:val_end]\n",
    "        else:  # TEST\n",
    "            split_indices = indices[val_end:]\n",
    "            \n",
    "        return [self.all_image_pairs[i] for i in split_indices]\n",
    "    \n",
    "    def save_split(self, output_file: Union[str, Path], is_exists: bool = False):\n",
    "        \"\"\"\n",
    "        Saves the current split configuration to a JSON file.\n",
    "        \n",
    "        Args:\n",
    "            output_file: Path to save the split configuration\n",
    "            is_exists: If file exist, add new split data\n",
    "        \"\"\"\n",
    "        if self.split_type:\n",
    "            split = self.split_type.value\n",
    "            split_info = {\n",
    "                'data' : {\n",
    "                    split: [str(p[0].relative_to(self.root_dir)) for p in self.image_pairs]\n",
    "                }\n",
    "            }\n",
    "            # Check if the file exists\n",
    "            if is_exists and Path(output_file).exists():\n",
    "                # Read the existing content\n",
    "                with open(output_file, 'r') as f:\n",
    "                    existing_data = json.load(f)\n",
    "                # Check if 'data' is already in the existing content, if not, create it\n",
    "                if 'data' not in existing_data:\n",
    "                    existing_data['data'] = {}\n",
    "                \n",
    "                # Add or update the split information\n",
    "                existing_data['data'][split] = split_info['data'][split]\n",
    "                split_info = existing_data\n",
    "            \n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(split_info, f, indent=2)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of image pairs in the dataset.\"\"\"\n",
    "        return len(self.image_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Retrieves the image pair at the given index.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the image pair to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: Processed (SAR image, optical image) pair\n",
    "        \"\"\"\n",
    "        # Get paths for SAR and optical images\n",
    "        s1_path, s2_path = self.image_pairs[idx]\n",
    "        \n",
    "        # Load images\n",
    "        s1_image = Image.open(s1_path).convert('RGB')\n",
    "        s2_image = Image.open(s2_path).convert('RGB')\n",
    "        \n",
    "        # Apply transforms\n",
    "        s1_image = self.transform(s1_image)\n",
    "        s2_image = self.transform(s2_image)\n",
    "        \n",
    "        return s1_image, s2_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af120347",
   "metadata": {
    "papermill": {
     "duration": 0.013396,
     "end_time": "2024-11-16T21:12:53.915704",
     "exception": false,
     "start_time": "2024-11-16T21:12:53.902308",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6582507b",
   "metadata": {
    "papermill": {
     "duration": 0.011476,
     "end_time": "2024-11-16T21:12:53.938766",
     "exception": false,
     "start_time": "2024-11-16T21:12:53.927290",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now, we can start with our implementation. First, we will implement our building blocks. Then, we’ll move on to implementing the generator and discriminator networks. Finally, we will combine them to form the complete architecture. Let’s get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e89be9",
   "metadata": {
    "papermill": {
     "duration": 0.011298,
     "end_time": "2024-11-16T21:12:53.961484",
     "exception": false,
     "start_time": "2024-11-16T21:12:53.950186",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Building Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734f2122",
   "metadata": {
    "papermill": {
     "duration": 0.011228,
     "end_time": "2024-11-16T21:12:53.984102",
     "exception": false,
     "start_time": "2024-11-16T21:12:53.972874",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The **Pix2Pix** architecture consists of several layers arranged in a specific order, which are then repeatedly used throughout the network. By implementing these layers as smaller, modular blocks, we avoid redundant code and achieve a more flexible, modular design. This modularity allows us to easily adjust the configuration of the architecture. \n",
    "\n",
    "There are two main types of blocks in the pix2pix architecture: the standard convolutional blocks and the upsampling blocks. The convolutional blocks are the usual blocks found in many convolutional networks, while the upsampling blocks use transpose convolution to increase the spatial dimensions, rather than reducing them.\n",
    "\n",
    "- **Convolutional blocks** consist of a Convolution Layer, a BatchNorm Layer, and a ReLU layer, in that order.\n",
    "- **Upsampling blocks** are similar but instead consist of a Transpose Convolution Layer, a BatchNorm Layer, a Dropout Layer, and a ReLU layer in that order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0d62fd",
   "metadata": {
    "papermill": {
     "duration": 0.014063,
     "end_time": "2024-11-16T21:12:54.009565",
     "exception": false,
     "start_time": "2024-11-16T21:12:53.995502",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For convolutional blocks, if padding is not used, the spatial dimensions usually decrease. Therefore, `DownsamplingBlock` is appropriate in my opinion, even when padding is used to keep the spatial resolution the same. This naming convention also works well with upsampling blocks. \n",
    "\n",
    "\n",
    "The module itself is straightforward:\n",
    "1. Convolution Layer\n",
    "2. BatchNorm\n",
    "3. ReLU\n",
    "\n",
    "In the pix2pix architecture, all downsampling blocks use the Leaky ReLU activation function with a slope of 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bc27be3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T21:12:54.041461Z",
     "iopub.status.busy": "2024-11-16T21:12:54.040836Z",
     "iopub.status.idle": "2024-11-16T21:12:54.049592Z",
     "shell.execute_reply": "2024-11-16T21:12:54.048687Z"
    },
    "papermill": {
     "duration": 0.025366,
     "end_time": "2024-11-16T21:12:54.051498",
     "exception": false,
     "start_time": "2024-11-16T21:12:54.026132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DownsamplingBlock(nn.Module):\n",
    "    \"\"\"Defines the Unet downsampling block. \n",
    "    \n",
    "    Consists of Convolution-BatchNorm-ReLU layer with k filters.\n",
    "    \"\"\"\n",
    "    def __init__(self, c_in, c_out, kernel_size=4, stride=2, \n",
    "                 padding=1, negative_slope=0.2, use_norm=True):\n",
    "        \"\"\"\n",
    "        Initializes the UnetDownsamplingBlock.\n",
    "        \n",
    "        Args:\n",
    "            c_in (int): The number of input channels.\n",
    "            c_out (int): The number of output channels.\n",
    "            kernel_size (int, optional): The size of the convolving kernel. Default is 4.\n",
    "            stride (int, optional): Stride of the convolution. Default is 2.\n",
    "            padding (int, optional): Zero-padding added to both sides of the input. Default is 0.\n",
    "            negative_slope (float, optional): Negative slope for the LeakyReLU activation function. Default is 0.2.\n",
    "            use_norm (bool, optinal): If use norm layer. If True add a BatchNorm layer after Conv. Default is True.\n",
    "        \"\"\"\n",
    "        super(DownsamplingBlock, self).__init__()\n",
    "        block = []\n",
    "        block += [nn.Conv2d(in_channels=c_in, out_channels=c_out,\n",
    "                          kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                          bias=(not use_norm) # No need to use a bias if there is a batchnorm layer after conv\n",
    "                          )]\n",
    "        if use_norm:\n",
    "            block += [nn.BatchNorm2d(num_features=c_out)]\n",
    "        \n",
    "        block += [nn.LeakyReLU(negative_slope=negative_slope)]\n",
    "\n",
    "        self.conv_block = nn.Sequential(*block)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv_block(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b205ee24",
   "metadata": {
    "papermill": {
     "duration": 0.01127,
     "end_time": "2024-11-16T21:12:54.074146",
     "exception": false,
     "start_time": "2024-11-16T21:12:54.062876",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The **upsampling block** is similar to the **downsampling block**, but it differs in three ways: \n",
    "1. It may include a dropout layer between the normalization layer and the activation function.\n",
    "2. The activation function is **ReLU**.\n",
    "3. Instead of a normal convolution, it uses **transpose convolution**.\n",
    "\n",
    "In the pix2pix architecture, some upsampling blocks use the dropout layer. To accommodate this, our implementation supports dropout.\n",
    "\n",
    "One important thing to note is that the upsampling block can use an upsample operation followed by a convolution layer, instead of transpose convolution. While the original architecture uses transpose convolutions, I’ve added the option to use the classic upsampling operation followed by a normal convolution. It turns out that transpose can sometimes cause checkerboard-like artifacts in the output. This change addresses this issue with transpose convolutions. For our dataset, transpose convolutions perform well, so this adjustment is not strictly necessary. However, it's useful to be aware of this option. \n",
    "\n",
    "For more details, I recommend checking out this lovely blog post from the Google Brain team:  \n",
    "[Deconvolution and Checkerboard Artifacts](https://distill.pub/2016/deconv-checkerboard/) at [distill.pub](https://distill.pub/)\n",
    "\n",
    "So, the upsampling block consists of:\n",
    "1. Transpose Convolution Layer\n",
    "2. BatchNorm\n",
    "3. Dropout (optional)\n",
    "4. ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0413737b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T21:12:54.098268Z",
     "iopub.status.busy": "2024-11-16T21:12:54.097956Z",
     "iopub.status.idle": "2024-11-16T21:12:54.107489Z",
     "shell.execute_reply": "2024-11-16T21:12:54.106717Z"
    },
    "papermill": {
     "duration": 0.024019,
     "end_time": "2024-11-16T21:12:54.109441",
     "exception": false,
     "start_time": "2024-11-16T21:12:54.085422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UpsamplingBlock(nn.Module):\n",
    "    \"\"\"Defines the Unet upsampling block.\n",
    "    \"\"\"\n",
    "    def __init__(self, c_in, c_out, kernel_size=4, stride=2, \n",
    "                 padding=1, use_dropout=False, use_upsampling=False, mode='nearest'):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initializes the Unet Upsampling Block.\n",
    "        \n",
    "        Args:\n",
    "            c_in (int): The number of input channels.\n",
    "            c_out (int): The number of output channels.\n",
    "            kernel_size (int, optional): Size of the convolving kernel. Default is 4.\n",
    "            stride (int, optional): Stride of the convolution. Default is 2.\n",
    "            padding (int, optional): Zero-padding added to both sides of the input. Default is 0.\n",
    "            use_dropout (bool, optional): if use dropout layers. Default is False.\n",
    "            upsample (bool, optinal): if use upsampling rather than transpose convolution. Default is False.\n",
    "            mode (str, optional): the upsampling algorithm: one of 'nearest', \n",
    "                'bilinear', 'bicubic'. Default: 'nearest'\n",
    "        \"\"\"\n",
    "        super(UpsamplingBlock, self).__init__()\n",
    "        block = []\n",
    "        if use_upsampling:\n",
    "            # Transpose convolution causes checkerboard artifacts. Upsampling\n",
    "            # followed by a regular convolutions produces better results appearantly\n",
    "            # Please check for further reading: https://distill.pub/2016/deconv-checkerboard/\n",
    "            # Odena, et al., \"Deconvolution and Checkerboard Artifacts\", Distill, 2016. http://doi.org/10.23915/distill.00003\n",
    "            \n",
    "            mode = mode if mode in ('nearest', 'bilinear', 'bicubic') else 'nearest'\n",
    "            \n",
    "            block += [nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2, mode=mode),\n",
    "                nn.Conv2d(in_channels=c_in, out_channels=c_out,\n",
    "                          kernel_size=3, stride=1, padding=padding,\n",
    "                          bias=False\n",
    "                          )\n",
    "                )]\n",
    "        else:\n",
    "            block += [nn.ConvTranspose2d(in_channels=c_in, \n",
    "                                         out_channels=c_out,\n",
    "                                         kernel_size=kernel_size, \n",
    "                                         stride=stride,\n",
    "                                         padding=padding, bias=False\n",
    "                                         )\n",
    "                     ]\n",
    "        \n",
    "        block += [nn.BatchNorm2d(num_features=c_out)]\n",
    "\n",
    "        if use_dropout:\n",
    "            block += [nn.Dropout(0.5)]\n",
    "            \n",
    "        block += [nn.ReLU()]\n",
    "\n",
    "        self.conv_block = nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_block(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8f3777",
   "metadata": {
    "papermill": {
     "duration": 0.014076,
     "end_time": "2024-11-16T21:12:54.134852",
     "exception": false,
     "start_time": "2024-11-16T21:12:54.120776",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99905e0b",
   "metadata": {
    "papermill": {
     "duration": 0.011966,
     "end_time": "2024-11-16T21:12:54.160202",
     "exception": false,
     "start_time": "2024-11-16T21:12:54.148236",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now that we’ve sorted out the building blocks, we can move on to implementing the network modules. **Pix2Pix** uses a **U-Net** architecture for its generator. The generator first reduces the input image to a low-dimensional latent space and then reconstructs it back to the original high-dimensional space. The discriminator is a **PatchGAN**. **PatchGAN** divides the input image into small patches and attempts to classify each patch as either real or fake. The discriminator is applied convolutionally across the entire image and then takes the average of all responses to produce the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f170a2",
   "metadata": {
    "papermill": {
     "duration": 0.012556,
     "end_time": "2024-11-16T21:12:54.184308",
     "exception": false,
     "start_time": "2024-11-16T21:12:54.171752",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7d12b6",
   "metadata": {
    "papermill": {
     "duration": 0.011206,
     "end_time": "2024-11-16T21:12:54.207728",
     "exception": false,
     "start_time": "2024-11-16T21:12:54.196522",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The generator follows a **U-Net**-like architecture. Key features of this architecture include the **encoder-decoder** structure and **skip connections**.\n",
    "\n",
    "1. The **encoder** is a subnetwork where the input image is encoded into a low-dimensional representation. \n",
    "2. The **decoder** is similar to the encoder but takes the low-dimensional representation and reconstructs it back to the original spatial dimensions.\n",
    "3. **Skip connections** transfer activations from corresponding layers in the encoder to the decoder.\n",
    "\n",
    "\n",
    "**Encoder**:\n",
    "\n",
    "The encoder consists of 8 downsampling blocks, with the following structure: \n",
    "- C64-C128-C256-C512-C512-C512-C512-C512\n",
    "- Convolutions use 4 × 4 filters with stride 2.\n",
    "- The encoder expects an input shape of (N,C,256,256).\n",
    "- At the end of the encoder, the output shape is (N,512,1,1).\n",
    "\n",
    "\n",
    "**Decoder**:\n",
    "\n",
    "The decoder consists of 8 upsampling blocks, with the following structure:\n",
    "- CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128\n",
    "- The number of channels is doubled in the decoder due to the skip connections.\n",
    "- The decoder expects an input shape of (N,512,1,1)\n",
    "- At the end of the decoder, the output shape is (N,64,256,256)\n",
    "\n",
    "After the last layer in the decoder, a convolution is applied to map to the desired number of output channels, followed by a **Tanh** activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64a74a93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T21:12:54.232658Z",
     "iopub.status.busy": "2024-11-16T21:12:54.231830Z",
     "iopub.status.idle": "2024-11-16T21:12:54.240859Z",
     "shell.execute_reply": "2024-11-16T21:12:54.240052Z"
    },
    "papermill": {
     "duration": 0.023611,
     "end_time": "2024-11-16T21:12:54.242793",
     "exception": false,
     "start_time": "2024-11-16T21:12:54.219182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UnetEncoder(nn.Module):\n",
    "    \"\"\"Create the Unet Encoder Network.\n",
    "    \n",
    "    C64-C128-C256-C512-C512-C512-C512-C512\n",
    "    \"\"\"\n",
    "    def __init__(self, c_in=3, c_out=512):\n",
    "        \"\"\"\n",
    "        Constructs the Unet Encoder Network.\n",
    "\n",
    "        Ck denote a Convolution-BatchNorm-ReLU layer with k filters.\n",
    "            C64-C128-C256-C512-C512-C512-C512-C512\n",
    "        Args:\n",
    "            c_in (int, optional): Number of input channels.\n",
    "            c_out (int, optional): Number of output channels. Default is 512.\n",
    "        \"\"\"\n",
    "        super(UnetEncoder, self).__init__()\n",
    "        self.enc1 = DownsamplingBlock(c_in, 64, use_norm=False) # C64\n",
    "        self.enc2 = DownsamplingBlock(64, 128) # C128\n",
    "        self.enc3 = DownsamplingBlock(128, 256) # C256\n",
    "        self.enc4 = DownsamplingBlock(256, 512) # C512\n",
    "        self.enc5 = DownsamplingBlock(512, 512) # C512\n",
    "        self.enc6 = DownsamplingBlock(512, 512) # C512\n",
    "        self.enc7 = DownsamplingBlock(512, 512) # C512\n",
    "        self.enc8 = DownsamplingBlock(512, c_out) # C512\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.enc1(x)\n",
    "        x2 = self.enc2(x1)\n",
    "        x3 = self.enc3(x2)\n",
    "        x4 = self.enc4(x3)\n",
    "        x5 = self.enc5(x4)\n",
    "        x6 = self.enc6(x5)\n",
    "        x7 = self.enc7(x6)\n",
    "        x8 = self.enc8(x7)\n",
    "        out = [x8, x7, x6, x5, x4, x3, x2, x1] # latest activation is the first element\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26b20542",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T21:12:54.267033Z",
     "iopub.status.busy": "2024-11-16T21:12:54.266736Z",
     "iopub.status.idle": "2024-11-16T21:12:54.278025Z",
     "shell.execute_reply": "2024-11-16T21:12:54.277259Z"
    },
    "papermill": {
     "duration": 0.025628,
     "end_time": "2024-11-16T21:12:54.279807",
     "exception": false,
     "start_time": "2024-11-16T21:12:54.254179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UnetDecoder(nn.Module):\n",
    "    \"\"\"Creates the Unet Decoder Network.\n",
    "    \"\"\"\n",
    "    def __init__(self, c_in=512, c_out=64, use_upsampling=False, mode='nearest'):\n",
    "        \"\"\"\n",
    "        Constructs the Unet Decoder Network.\n",
    "\n",
    "        Ck denote a Convolution-BatchNorm-ReLU layer with k filters.\n",
    "        \n",
    "        CDk denotes a Convolution-BatchNorm-Dropout-ReLU layer with a dropout rate of 50%.\n",
    "            CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128\n",
    "        Args:\n",
    "            c_in (int): Number of input channels.\n",
    "            c_out (int, optional): Number of output channels. Default is 512.\n",
    "            use_upsampling (bool, optional): Upsampling method for decoder. \n",
    "                If True, use upsampling layer followed regular convolution layer.\n",
    "                If False, use transpose convolution. Default is False\n",
    "            mode (str, optional): the upsampling algorithm: one of 'nearest', \n",
    "                'bilinear', 'bicubic'. Default: 'nearest'\n",
    "        \"\"\"\n",
    "        super(UnetDecoder, self).__init__()\n",
    "        self.dec1 = UpsamplingBlock(c_in, 512, use_dropout=True, use_upsampling=use_upsampling, mode=mode) # CD512\n",
    "        self.dec2 = UpsamplingBlock(1024, 512, use_dropout=True, use_upsampling=use_upsampling, mode=mode) # CD1024\n",
    "        self.dec3 = UpsamplingBlock(1024, 512, use_dropout=True, use_upsampling=use_upsampling, mode=mode) # CD1024\n",
    "        self.dec4 = UpsamplingBlock(1024, 512, use_upsampling=use_upsampling, mode=mode) # C1024\n",
    "        self.dec5 = UpsamplingBlock(1024, 256, use_upsampling=use_upsampling, mode=mode) # C1024\n",
    "        self.dec6 = UpsamplingBlock(512, 128, use_upsampling=use_upsampling, mode=mode) # C512\n",
    "        self.dec7 = UpsamplingBlock(256, 64, use_upsampling=use_upsampling, mode=mode) # C256\n",
    "        self.dec8 = UpsamplingBlock(128, c_out, use_upsampling=use_upsampling, mode=mode) # C128\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x9 = torch.cat([x[1], self.dec1(x[0])], 1) # (N,1024,H,W)\n",
    "        x10 = torch.cat([x[2], self.dec2(x9)], 1) # (N,1024,H,W)\n",
    "        x11 = torch.cat([x[3], self.dec3(x10)], 1) # (N,1024,H,W)\n",
    "        x12 = torch.cat([x[4], self.dec4(x11)], 1) # (N,1024,H,W)\n",
    "        x13 = torch.cat([x[5], self.dec5(x12)], 1) # (N,512,H,W)\n",
    "        x14 = torch.cat([x[6], self.dec6(x13)], 1) # (N,256,H,W)\n",
    "        x15 = torch.cat([x[7], self.dec7(x14)], 1) # (N,128,H,W)\n",
    "        out = self.dec8(x15) # (N,64,H,W)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47e473ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T21:12:54.304369Z",
     "iopub.status.busy": "2024-11-16T21:12:54.303807Z",
     "iopub.status.idle": "2024-11-16T21:12:54.311751Z",
     "shell.execute_reply": "2024-11-16T21:12:54.310918Z"
    },
    "papermill": {
     "duration": 0.022208,
     "end_time": "2024-11-16T21:12:54.313630",
     "exception": false,
     "start_time": "2024-11-16T21:12:54.291422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UnetGenerator(nn.Module):\n",
    "    \"\"\"Create a Unet-based generator\"\"\"\n",
    "    def __init__(self, c_in=3, c_out=3, use_upsampling=False, mode='nearest'):\n",
    "        \"\"\"\n",
    "        Constructs a Unet generator\n",
    "        Args:\n",
    "            c_in (int): The number of input channels.\n",
    "            c_out (int): The number of output channels.\n",
    "            use_upsampling (bool, optional): Upsampling method for decoder. \n",
    "                If True, use upsampling layer followed regular convolution layer.\n",
    "                If False, use transpose convolution. Default is False\n",
    "            mode (str, optional): the upsampling algorithm: one of 'nearest', \n",
    "                'bilinear', 'bicubic'. Default: 'nearest'\n",
    "        \"\"\"\n",
    "        super(UnetGenerator, self).__init__()\n",
    "        self.encoder = UnetEncoder(c_in=c_in)\n",
    "        self.decoder = UnetDecoder(use_upsampling=use_upsampling, mode=mode)\n",
    "        # In the paper, the authors state:\n",
    "        #   \"\"\"\n",
    "        #       After the last layer in the decoder, a convolution is applied\n",
    "        #       to map to the number of output channels (3 in general, except\n",
    "        #       in colorization, where it is 2), followed by a Tanh function.\n",
    "        #   \"\"\"\n",
    "        # However, in the official Lua implementation, only a Tanh layer is applied.\n",
    "        # Therefore, I took the liberty of adding a convolutional layer with a \n",
    "        # kernel size of 3.\n",
    "        # For more information please check the paper and official github repo:\n",
    "        # https://github.com/phillipi/pix2pix\n",
    "        # https://arxiv.org/abs/1611.07004\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=c_out,\n",
    "                      kernel_size=3, stride=1, padding=1,\n",
    "                      bias=True\n",
    "                      ), \n",
    "            nn.Tanh()\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outE = self.encoder(x)\n",
    "        outD = self.decoder(outE)\n",
    "        out = self.head(outD)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e1bd52",
   "metadata": {
    "papermill": {
     "duration": 0.011242,
     "end_time": "2024-11-16T21:12:54.336279",
     "exception": false,
     "start_time": "2024-11-16T21:12:54.325037",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddd4c7c",
   "metadata": {
    "papermill": {
     "duration": 0.01121,
     "end_time": "2024-11-16T21:12:54.358934",
     "exception": false,
     "start_time": "2024-11-16T21:12:54.347724",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As previously mentioned, the discriminator in **Pix2Pix** is a **PatchGAN**. **PatchGAN** divides the input image into small patches and classifies each patch as either real or fake. This is achieved by applying a series of convolutional blocks, rather than evaluating the entire patch at once. With each convolution, the receptive field of the network grows, allowing it to make decisions based on increasingly larger portions of the patch. As a result, the number of layers in the **PatchGAN** varies depending on the patch size. In the original Pix2Pix paper, the authors found that a **70x70 PatchGAN** performs best.\n",
    "\n",
    "For more details about receptive fields, I recommend checking out another excellent blog from [distill.pub](https://distill.pub/):  \n",
    "[Computing Receptive Fields of Convolutional Neural Networks](https://distill.pub/2019/computing-receptive-fields/)\n",
    "\n",
    "A special case of **PatchGAN** is **PixelGAN**, where all convolutions are 1x1.\n",
    "\n",
    "**Discriminator Architectures**:\n",
    "\n",
    "- 70 × 70 discriminator architecture is:\n",
    "C64-C128-C256-C512\n",
    "\n",
    "- 1 × 1 discriminator:\n",
    "C64-C128\n",
    "\n",
    "- 16 × 16 discriminator:\n",
    "C64-C128\n",
    "\n",
    "- 286 × 286 discriminator:\n",
    "C64-C128-C256-C512-C512-C512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a6e5186",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T21:12:54.383112Z",
     "iopub.status.busy": "2024-11-16T21:12:54.382548Z",
     "iopub.status.idle": "2024-11-16T21:12:54.389249Z",
     "shell.execute_reply": "2024-11-16T21:12:54.388441Z"
    },
    "papermill": {
     "duration": 0.020834,
     "end_time": "2024-11-16T21:12:54.391099",
     "exception": false,
     "start_time": "2024-11-16T21:12:54.370265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PixelDiscriminator(nn.Module):\n",
    "    \"\"\"Create a PixelGAN discriminator (1x1 PatchGAN discriminator)\"\"\"\n",
    "    def __init__(self, c_in=3, c_hid=64):\n",
    "        \"\"\"Constructs a PixelGAN discriminator, a special form of PatchGAN Discriminator.\n",
    "        All convolutions are 1x1 spatial filters\n",
    "\n",
    "        Args:\n",
    "            c_in (int, optional): The number of input channels. Defaults to 3.\n",
    "            c_hid (int, optional): The number of channels after first conv layer.\n",
    "                Defaults to 64.\n",
    "        \"\"\"\n",
    "        super(PixelDiscriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            DownsamplingBlock(c_in, c_hid, kernel_size=1, stride=1, padding=0, use_norm=False),\n",
    "            DownsamplingBlock(c_hid, c_hid*2, kernel_size=1, stride=1, padding=0),\n",
    "            nn.Conv2d(in_channels=c_hid*2, out_channels=1, kernel_size=1)\n",
    "            )\n",
    "        # Similar to PatchDiscriminator, there should be a sigmoid layer at the end of discriminator.\n",
    "        # However, nn.BCEWithLogitsLoss combines the sigmoid layer with BCE loss, \n",
    "        # providing greater numerical stability. Therefore, the discriminator outputs\n",
    "        # logits to take advantage of this stability.\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2d1a554",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T21:12:54.415413Z",
     "iopub.status.busy": "2024-11-16T21:12:54.414844Z",
     "iopub.status.idle": "2024-11-16T21:12:54.423480Z",
     "shell.execute_reply": "2024-11-16T21:12:54.422707Z"
    },
    "papermill": {
     "duration": 0.022847,
     "end_time": "2024-11-16T21:12:54.425436",
     "exception": false,
     "start_time": "2024-11-16T21:12:54.402589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchDiscriminator(nn.Module):\n",
    "    \"\"\"Create a PatchGAN discriminator\"\"\"\n",
    "    def __init__(self, c_in=3, c_hid=64, n_layers=3):\n",
    "        \"\"\"Constructs a PatchGAN discriminator\n",
    "\n",
    "        Args:\n",
    "            c_in (int, optional): The number of input channels. Defaults to 3.\n",
    "            c_hid (int, optional): The number of channels after first conv layer.\n",
    "                Defaults to 64.\n",
    "            n_layers (int, optional): the number of convolution blocks in the \n",
    "                discriminator. Defaults to 3.\n",
    "        \"\"\"\n",
    "        super(PatchDiscriminator, self).__init__()\n",
    "        model = [DownsamplingBlock(c_in, c_hid, use_norm=False)]\n",
    "\n",
    "        n_p = 1  # multiplier for previous channel\n",
    "        n_c = 1  # multiplier for current channel\n",
    "        # last block is with stride of 1, therefore iterate (n_layers-1) times\n",
    "        for n in range(1, n_layers): \n",
    "            n_p = n_c\n",
    "            n_c = min(2**n, 8)  # The number of channels is 512 at most\n",
    "\n",
    "            model += [DownsamplingBlock(c_hid*n_p, c_hid*n_c)]\n",
    "        \n",
    "        n_p = n_c\n",
    "        n_c = min(2**n_layers, 8)\n",
    "        model += [DownsamplingBlock(c_hid*n_p, c_hid*n_c, stride=1)] # last block is with stride of 1\n",
    "\n",
    "        # last layer is a convolution followed by a Sigmoid function.\n",
    "        model += [nn.Conv2d(in_channels=c_hid*n_c, out_channels=1, \n",
    "                            kernel_size=4, stride=1, padding=1, bias=True\n",
    "                            )] \n",
    "        # Normally, there should be a sigmoid layer at the end of discriminator.\n",
    "        # However, nn.BCEWithLogitsLoss combines the sigmoid layer with BCE loss, \n",
    "        # providing greater numerical stability. Therefore, the discriminator outputs\n",
    "        # logits to take advantage of this stability.\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9844a6ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T21:12:54.450185Z",
     "iopub.status.busy": "2024-11-16T21:12:54.449505Z",
     "iopub.status.idle": "2024-11-16T21:12:54.456416Z",
     "shell.execute_reply": "2024-11-16T21:12:54.455617Z"
    },
    "papermill": {
     "duration": 0.021359,
     "end_time": "2024-11-16T21:12:54.458303",
     "exception": false,
     "start_time": "2024-11-16T21:12:54.436944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchGAN(nn.Module):\n",
    "    \"\"\"Create a PatchGAN discriminator\"\"\"\n",
    "    def __init__(self, c_in=3, c_hid=64, mode='patch', n_layers=3):\n",
    "        \"\"\"Constructs a PatchGAN discriminator.\n",
    "\n",
    "        Args:\n",
    "            c_in (int, optional): The number of input channels. Defaults to 3.\n",
    "            c_hid (int, optional): The number of channels after first \n",
    "                convolutional layer. Defaults to 64.\n",
    "            mode (str, optional): PatchGAN type. Use 'pixel' for PixelGAN, and \n",
    "                'patch' for other types. Defaults to 'patch'.\n",
    "            n_layers (int, optional): PatchGAN number of layers. Defaults to 3.\n",
    "                - 16x16 PatchGAN if n=1\n",
    "                - 34x34 PatchGAN if n=2\n",
    "                - 70x70 PatchGAN if n=3\n",
    "                - 142x142 PatchGAN if n=4\n",
    "                - 286x286 PatchGAN if n=5\n",
    "                - 574x574 PatchGAN if n=6\n",
    "        \"\"\"\n",
    "        super(PatchGAN, self).__init__()\n",
    "        if mode == 'pixel':\n",
    "            self.model = PixelDiscriminator(c_in, c_hid)\n",
    "        else:\n",
    "            self.model = PatchDiscriminator(c_in, c_hid, n_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d1892d",
   "metadata": {
    "papermill": {
     "duration": 0.011155,
     "end_time": "2024-11-16T21:12:54.480815",
     "exception": false,
     "start_time": "2024-11-16T21:12:54.469660",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Pix2Pix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56efff6d",
   "metadata": {
    "papermill": {
     "duration": 0.011221,
     "end_time": "2024-11-16T21:12:54.503339",
     "exception": false,
     "start_time": "2024-11-16T21:12:54.492118",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "It's time to combine everything to build the Pix2Pix network. The Pix2Pix module consists of a generator and a discriminator. However, it has additional functionalities to make things easier and cleaner. Since we are typically interested only in the generator, we initialize the discriminator only if we want to train the model. The Pix2Pix class takes care of preparing the losses and optimizers for us. We use the classic BCE Loss for adversarial training, and L1 loss is added additionally for the generator to help it produce higher quality images. The class also provides some additional methods for tasks like weight initialization and performing a training step. GANs are extremely sensitive to initialization. The parameters we used in the method are typically works well, and is also used by author of the pix2pix model. We have methods for performing both a single training step and a validation step. During the training step, the generator creates fake images, the discriminator is updated, and then the generator is updated. In the validation step, the same process occurs, but neither the generator nor the discriminator is updated; only the losses are computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b95f946",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T21:12:54.527686Z",
     "iopub.status.busy": "2024-11-16T21:12:54.527391Z",
     "iopub.status.idle": "2024-11-16T21:12:54.567815Z",
     "shell.execute_reply": "2024-11-16T21:12:54.566965Z"
    },
    "papermill": {
     "duration": 0.055038,
     "end_time": "2024-11-16T21:12:54.569755",
     "exception": false,
     "start_time": "2024-11-16T21:12:54.514717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Pix2Pix(nn.Module):\n",
    "    \"\"\"Create a Pix2Pix class. It is a model for image to image translation tasks.\n",
    "    By default, the model uses a Unet architecture for generator with transposed\n",
    "    convolution. The discriminator is 70x70 PatchGAN discriminator, by default.\n",
    "     \"\"\"\n",
    "    def __init__(self, \n",
    "                 c_in: int = 3, \n",
    "                 c_out: int = 3, \n",
    "                 is_train: bool = True,\n",
    "                 netD: str = 'patch',\n",
    "                 lambda_L1: float = 100.0,\n",
    "                 is_CGAN: bool = True,\n",
    "                 use_upsampling: bool = False,\n",
    "                 mode: str = 'nearest',\n",
    "                 c_hid: int = 64,\n",
    "                 n_layers: int = 3,\n",
    "                 lr: float = 0.0002,\n",
    "                 beta1: float = 0.5,\n",
    "                 beta2: float = 0.999\n",
    "                 ):\n",
    "        \"\"\"Constructs the Pix2Pix class.\n",
    "        \n",
    "        Args:\n",
    "            c_in: Number of input channels\n",
    "            c_out: Number of output channels\n",
    "            is_train: Whether the model is in training mode\n",
    "            netD: Type of discriminator ('patch' or 'pixel')\n",
    "            lambda_L1: Weight for L1 loss\n",
    "            is_CGAN: If True, use conditional GAN architecture\n",
    "            use_upsampling: If True, use upsampling in generator instead of transpose conv\n",
    "            mode: Upsampling mode ('nearest', 'bilinear', 'bicubic')\n",
    "            c_hid: Number of base filters in discriminator\n",
    "            n_layers: Number of layers in discriminator\n",
    "            lr: Learning rate\n",
    "            beta1: Beta1 parameter for Adam optimizer\n",
    "            beta2: Beta2 parameter for Adam optimizer\n",
    "        \"\"\"\n",
    "        super(Pix2Pix, self).__init__()\n",
    "        self.is_CGAN = is_CGAN\n",
    "        self.lambda_L1 = lambda_L1\n",
    "        self.is_train = is_train\n",
    "\n",
    "        self.gen = UnetGenerator(c_in=c_in, c_out=c_out, use_upsampling=use_upsampling, mode=mode)\n",
    "        self.gen = self.gen.apply(self.weights_init)\n",
    "        \n",
    "        if self.is_train:\n",
    "            # Conditional GANs need both input and output together, the total input channel is c_in+c_out\n",
    "            disc_in = c_in + c_out if is_CGAN else c_out\n",
    "            self.disc = PatchGAN(c_in=disc_in, c_hid=c_hid, mode=netD, n_layers=n_layers) \n",
    "            self.disc = self.disc.apply(self.weights_init)\n",
    "\n",
    "            # Initialize optimizers\n",
    "            self.gen_optimizer = torch.optim.Adam(\n",
    "                self.gen.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "            self.disc_optimizer = torch.optim.Adam(\n",
    "                self.disc.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "            # Initialize loss functions\n",
    "            self.criterion = nn.BCEWithLogitsLoss()\n",
    "            self.criterion_L1 = nn.L1Loss()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.gen(x)\n",
    "    \n",
    "    @staticmethod    \n",
    "    def weights_init(m):\n",
    "        \"\"\"Initialize network weights.\n",
    "        \n",
    "        Args:\n",
    "            m: network module\n",
    "        \"\"\"\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "            nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _get_disc_inputs(self, \n",
    "                         real_images: torch.Tensor,\n",
    "                         target_images: torch.Tensor, \n",
    "                         fake_images: torch.Tensor\n",
    "                         ):\n",
    "        \"\"\"Prepare discriminator inputs based on conditional/unconditional setup.\"\"\"\n",
    "        if self.is_CGAN:\n",
    "            # Conditional GANs need both input and output together, \n",
    "            # Therefore, the total input channel is c_in+c_out\n",
    "            real_AB = torch.cat([real_images, target_images], dim=1)\n",
    "            fake_AB = torch.cat([real_images, \n",
    "                               fake_images.detach()], \n",
    "                               dim=1)\n",
    "        else:\n",
    "            real_AB = target_images\n",
    "            fake_AB = fake_images.detach()\n",
    "        return real_AB, fake_AB\n",
    "    \n",
    "    def _get_gen_inputs(self, \n",
    "                        real_images: torch.Tensor, \n",
    "                        fake_images: torch.Tensor\n",
    "                        ):\n",
    "        \"\"\"Prepare discriminator inputs based on conditional/unconditional setup.\"\"\"\n",
    "        if self.is_CGAN:\n",
    "            # Conditional GANs need both input and output together, \n",
    "            # Therefore, the total input channel is c_in+c_out\n",
    "            fake_AB = torch.cat([real_images, \n",
    "                               fake_images], \n",
    "                               dim=1)\n",
    "        else:\n",
    "            fake_AB = fake_images\n",
    "        return fake_AB\n",
    "    \n",
    "    \n",
    "    def step_discriminator(self, \n",
    "                           real_images: torch.Tensor, \n",
    "                           target_images: torch.Tensor, \n",
    "                           fake_images: torch.Tensor\n",
    "                           ):\n",
    "        \"\"\"Discriminator forward/backward pass.\n",
    "        \n",
    "        Args:\n",
    "            real_images: Input images\n",
    "            target_images: Ground truth images\n",
    "            fake_images: Generated images\n",
    "            \n",
    "        Returns:\n",
    "            Discriminator loss value\n",
    "        \"\"\"\n",
    "        # Prepare inputs\n",
    "        real_AB, fake_AB = self._get_disc_inputs(real_images, target_images, \n",
    "                                                fake_images)\n",
    "          \n",
    "        # Forward pass through the discriminator\n",
    "        pred_real = self.disc(real_AB) # D(x, y)\n",
    "        pred_fake = self.disc(fake_AB) # D(x, G(x))\n",
    "\n",
    "        # Compute the losses\n",
    "        lossD_real = self.criterion(pred_real, torch.ones_like(pred_real)) # (D(x, y), 1)\n",
    "        lossD_fake = self.criterion(pred_fake, torch.zeros_like(pred_fake)) # (D(x, y), 0)\n",
    "        lossD = (lossD_real + lossD_fake) * 0.5 # Combined Loss\n",
    "        return lossD\n",
    "    \n",
    "    def step_generator(self, \n",
    "                       real_images: torch.Tensor, \n",
    "                       target_images: torch.Tensor, \n",
    "                       fake_images: torch.Tensor\n",
    "                       ):\n",
    "        \"\"\"Discriminator forward/backward pass.\n",
    "        \n",
    "        Args:\n",
    "            real_images: Input images\n",
    "            target_images: Ground truth images\n",
    "            fake_images: Generated images\n",
    "            \n",
    "        Returns:\n",
    "            Discriminator loss value\n",
    "        \"\"\"\n",
    "        # Prepare input\n",
    "        fake_AB = self._get_gen_inputs(real_images, fake_images)\n",
    "          \n",
    "        # Forward pass through the discriminator\n",
    "        pred_fake = self.disc(fake_AB)\n",
    "\n",
    "        # Compute the losses\n",
    "        lossG_GaN = self.criterion(pred_fake, torch.ones_like(pred_fake)) # GAN Loss\n",
    "        lossG_L1 = self.criterion_L1(fake_images, target_images)           # L1 Loss\n",
    "        lossG = lossG_GaN + self.lambda_L1 * lossG_L1                      # Combined Loss\n",
    "        # Return total loss and individual components\n",
    "        return lossG, {\n",
    "            'loss_G': lossG.item(),\n",
    "            'loss_G_GAN': lossG_GaN.item(),\n",
    "            'loss_G_L1': lossG_L1.item()\n",
    "        }\n",
    "    \n",
    "    def train_step(self, \n",
    "                   real_images: torch.Tensor, \n",
    "                   target_images: torch.Tensor\n",
    "                   ):\n",
    "        \"\"\"Performs a single training step.\n",
    "        \n",
    "        Args:\n",
    "            real_images: Input images\n",
    "            target_images: Ground truth images\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing all loss values from this step\n",
    "        \"\"\"\n",
    "        # Forward pass through the generator\n",
    "        fake_images = self.forward(real_images)\n",
    "        \n",
    "        # Update discriminator\n",
    "        self.disc_optimizer.zero_grad() # Reset the gradients for D\n",
    "        lossD = self.step_discriminator(real_images, target_images, fake_images) # Compute the loss\n",
    "        lossD.backward()\n",
    "        self.disc_optimizer.step() # Update D\n",
    "\n",
    "        # Update generator\n",
    "        self.gen_optimizer.zero_grad() # Reset the gradients for D\n",
    "        lossG, G_losses = self.step_generator(real_images, target_images, fake_images) # Compute the loss\n",
    "        lossG.backward()\n",
    "        self.gen_optimizer.step() # Update D\n",
    "\n",
    "        # Return all losses\n",
    "        return {\n",
    "            'loss_D': lossD.item(),\n",
    "            **G_losses\n",
    "        }\n",
    "    \n",
    "    def validation_step(self, \n",
    "                   real_images: torch.Tensor, \n",
    "                   target_images: torch.Tensor\n",
    "                   ):\n",
    "        \"\"\"Performs a single validation step.\n",
    "        \n",
    "        Args:\n",
    "            real_images: Input images\n",
    "            target_images: Ground truth images\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing all loss values from this step\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Forward pass through the generator\n",
    "            fake_images = self.forward(real_images)\n",
    "\n",
    "            # Compute the loss for D\n",
    "            lossD = self.step_discriminator(real_images, target_images, fake_images)\n",
    "            \n",
    "            # Compute the loss for G\n",
    "            _, G_losses = self.step_generator(real_images, target_images, fake_images)\n",
    "\n",
    "        # Return all losses\n",
    "        return {\n",
    "            'loss_D': lossD.item(),\n",
    "            **G_losses\n",
    "        }\n",
    "    \n",
    "    def generate(self, \n",
    "                 real_images: torch.Tensor, \n",
    "                 is_scaled: bool = False, \n",
    "                 to_uint8: bool = False\n",
    "                 ):\n",
    "        if not is_scaled:\n",
    "            real_images = real_images.to(dtype=torch.float32) # Make sure it's a float tensor\n",
    "            real_images = real_images / 255.0 # Normalize to [0, 1]\n",
    "        real_images = (real_images - 0.5) / 0.5 # Scale to [-1, 1]\n",
    "\n",
    "        with torch.no_grad(): # generate image\n",
    "            generated_images = self.forward(real_images)\n",
    "\n",
    "        generated_images = (generated_images + 1) / 2  # Rescale to [0, 1]\n",
    "        if to_uint8:\n",
    "            generated_images = (generated_images* 255).to(dtype=torch.uint8)  # Scale to [0, 255] and convert to uint8\n",
    "        \n",
    "        return generated_images\n",
    "\n",
    "            \n",
    "    def save_model(self, gen_path: str, disc_path: str = None):\n",
    "        \"\"\"\n",
    "        Saves the generator model's state dictionary to the specified path.\n",
    "        If in training mode and a discriminator path is provided, saves the\n",
    "        discriminator model's state dictionary as well.\n",
    "\n",
    "        Args:\n",
    "            gen_path (str): The file path where the generator model's state dictionary will be saved.\n",
    "            disc_path (str, optional): The file path where the discriminator model's state dictionary will be saved. Defaults to None.\n",
    "        \"\"\"\n",
    "        torch.save(self.gen.state_dict(), gen_path)\n",
    "        if self.is_train and disc_path is not None:\n",
    "            torch.save(self.disc.state_dict(), disc_path)\n",
    "    \n",
    "    def load_model(self, gen_path: str, disc_path: str = None, device: str = None):\n",
    "        \"\"\"\n",
    "        Loads the generator and optionally the discriminator model from the specified file paths.\n",
    "\n",
    "        Args:\n",
    "            gen_path (str): Path to the generator model file.\n",
    "            disc_path (str, optional): Path to the discriminator model file. Defaults to None.\n",
    "            device (torch.device, optional): The device on which to load the models. If None, the device of the model's parameters will be used. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        device = device if device else next(self.gen.parameters()).device\n",
    "        self.gen.load_state_dict(torch.load(gen_path, map_location=device, weights_only=True), strict=False)\n",
    "        if disc_path is not None and self.is_train:\n",
    "            device = device if device else next(self.disc.parameters()).device\n",
    "            self.disc.load_state_dict(torch.load(gen_path, map_location=device, weights_only=True), strict=False)\n",
    "    \n",
    "    def save_optimizer(self, gen_opt_path: str, disc_opt_path: str = None):\n",
    "        \"\"\"\n",
    "        Save the state of the optimizers to the specified file paths.\n",
    "        Args:\n",
    "            gen_opt_path (str): The file path to save the generator optimizer state.\n",
    "            disc_opt_path (str, optional): The file path to save the discriminator optimizer state. Defaults to None.\n",
    "        Notes:\n",
    "            This method saves the state of the generator optimizer to the specified `gen_opt_path`.\n",
    "            If `disc_opt_path` is provided, it also saves the state of the discriminator optimizer to the specified path.\n",
    "            This method only works if the model is in training mode (`is_train` is True). If the model is not in training mode,\n",
    "            it will print a message indicating that the model is not initialized in train mode.\n",
    "        \"\"\"\n",
    "        if self.is_train:\n",
    "            torch.save(self.gen_optimizer.state_dict(), gen_opt_path)\n",
    "            if disc_opt_path is not None:\n",
    "                torch.save(self.disc_optimizer.state_dict(), disc_opt_path)\n",
    "        else:\n",
    "            print('Model is initialized in train mode. See `is_train` for more.')\n",
    "    \n",
    "    def load_optimizer(self, gen_opt_path: str, disc_opt_path: str = None):\n",
    "        \"\"\"\n",
    "        Loads the optimizer states for the generator and discriminator from the specified file paths.\n",
    "        Args:\n",
    "            gen_opt_path (str): Path to the file containing the generator optimizer state.\n",
    "            disc_opt_path (str, optional): Path to the file containing the discriminator optimizer state. Defaults to None.\n",
    "        Notes:\n",
    "            This method saves the state of the generator optimizer to the specified `gen_opt_path`.\n",
    "            If `disc_opt_path` is provided, it also saves the state of the discriminator optimizer to the specified path.\n",
    "            This method only works if the model is in training mode (`is_train` is True). If the model is not in training mode,\n",
    "            it will print a message indicating that the model is not initialized in train mode.\n",
    "        \"\"\"\n",
    "        if self.is_train:\n",
    "            self.gen_optimizer.load_state_dict(torch.load(gen_opt_path, weights_only=True))\n",
    "            if disc_opt_path is not None:\n",
    "                self.disc_optimizer.load_state_dict(torch.load(disc_opt_path, weights_only=True))\n",
    "        else:\n",
    "            print('Model is initialized in train mode. See `is_train` for more.')\n",
    "    \n",
    "    def get_current_visuals(self, \n",
    "                            real_images: torch.Tensor, \n",
    "                            target_images: torch.Tensor\n",
    "                            ):\n",
    "        \"\"\"Return visualization images.\n",
    "        \n",
    "        Args:\n",
    "            real_images: Input images\n",
    "            target_images: Ground truth images\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing input, target and generated images\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            fake_images = self.gen(real_images)\n",
    "        return {\n",
    "            'real': real_images,\n",
    "            'fake': fake_images,\n",
    "            'target': target_images\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505a2cd6",
   "metadata": {
    "papermill": {
     "duration": 0.011238,
     "end_time": "2024-11-16T21:12:54.592367",
     "exception": false,
     "start_time": "2024-11-16T21:12:54.581129",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2574f8d",
   "metadata": {
    "papermill": {
     "duration": 0.011433,
     "end_time": "2024-11-16T21:12:54.615197",
     "exception": false,
     "start_time": "2024-11-16T21:12:54.603764",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Set Up Logging For Experimentation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c706b04",
   "metadata": {
    "papermill": {
     "duration": 0.011424,
     "end_time": "2024-11-16T21:12:54.637961",
     "exception": false,
     "start_time": "2024-11-16T21:12:54.626537",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Comet ML provides a convenient API to log experiments in a way easy to follow. It is also free for individuals. There are also other choices, you can replace it with your choice of platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deeabd5",
   "metadata": {
    "papermill": {
     "duration": 0.011492,
     "end_time": "2024-11-16T21:12:54.660798",
     "exception": false,
     "start_time": "2024-11-16T21:12:54.649306",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If Comet api is not installed, run the following line to install it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "293a29ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T21:12:54.685347Z",
     "iopub.status.busy": "2024-11-16T21:12:54.684963Z",
     "iopub.status.idle": "2024-11-16T21:13:22.258143Z",
     "shell.execute_reply": "2024-11-16T21:13:22.256547Z"
    },
    "papermill": {
     "duration": 27.587827,
     "end_time": "2024-11-16T21:13:22.260239",
     "exception": false,
     "start_time": "2024-11-16T21:12:54.672412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install comet_ml -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad0bc83",
   "metadata": {
    "papermill": {
     "duration": 0.01138,
     "end_time": "2024-11-16T21:13:22.283562",
     "exception": false,
     "start_time": "2024-11-16T21:13:22.272182",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Login using your api key.\n",
    "\n",
    "Do not hardcode you api key, use an configuration file or use secrets if you are using colabs or kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c88317b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T21:13:22.308758Z",
     "iopub.status.busy": "2024-11-16T21:13:22.307986Z",
     "iopub.status.idle": "2024-11-16T21:13:23.581227Z",
     "shell.execute_reply": "2024-11-16T21:13:23.580212Z"
    },
    "papermill": {
     "duration": 1.288238,
     "end_time": "2024-11-16T21:13:23.583388",
     "exception": false,
     "start_time": "2024-11-16T21:13:22.295150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Valid Comet API Key saved in /root/.comet.config (set COMET_CONFIG to change where it is saved).\n"
     ]
    }
   ],
   "source": [
    "# Log in to comet ml for experiment logging\n",
    "# Uncomment depending on the platform you are running: Kaggle, Colab\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "#from google.colab import userdata\n",
    "\n",
    "import comet_ml\n",
    "\n",
    "api_key = UserSecretsClient().get_secret('COMET_API_KEY')\n",
    "#api_key = userdata.get('COMET_API_KEY')\n",
    "\n",
    "# If you are running on local, run `comet_ml.login()` line only, it will prompt \n",
    "# you for your API key which, once provided, is automatically accessed from a \n",
    "# configuration file\n",
    "comet_ml.login(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22f0a5c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T21:13:23.608648Z",
     "iopub.status.busy": "2024-11-16T21:13:23.607879Z",
     "iopub.status.idle": "2024-11-16T21:13:24.912142Z",
     "shell.execute_reply": "2024-11-16T21:13:24.910954Z"
    },
    "papermill": {
     "duration": 1.318921,
     "end_time": "2024-11-16T21:13:24.914230",
     "exception": false,
     "start_time": "2024-11-16T21:13:23.595309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/yuulind/sar-2-optical/82525ff1312e4019a566c00a209959c1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create experiment object\n",
    "#experiment = comet_ml.start(project_name=\"SAR-2-Optical\")\n",
    "experiment = comet_ml.start(mode=\"get\", experiment_key=\"82525ff1312e4019a566c00a209959c1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c11deb",
   "metadata": {
    "papermill": {
     "duration": 0.01172,
     "end_time": "2024-11-16T21:13:24.938091",
     "exception": false,
     "start_time": "2024-11-16T21:13:24.926371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e29171b",
   "metadata": {
    "papermill": {
     "duration": 0.011691,
     "end_time": "2024-11-16T21:13:24.961809",
     "exception": false,
     "start_time": "2024-11-16T21:13:24.950118",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "First step in training is to define hyperparameters. Then, we log them using the experiment object we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c55d3a75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T21:13:24.987081Z",
     "iopub.status.busy": "2024-11-16T21:13:24.986749Z",
     "iopub.status.idle": "2024-11-16T21:13:25.128503Z",
     "shell.execute_reply": "2024-11-16T21:13:25.127518Z"
    },
    "papermill": {
     "duration": 0.156626,
     "end_time": "2024-11-16T21:13:25.130540",
     "exception": false,
     "start_time": "2024-11-16T21:13:24.973914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    'netD' : 'patch',\n",
    "    'lambda_L1' : 100.0,\n",
    "    'is_CGAN' : True,\n",
    "    'use_upsampling' : False,\n",
    "    'mode' : 'nearest',\n",
    "    'c_hid' : 64,\n",
    "    'n_layers' : 3,\n",
    "    'lr' : 0.0002,\n",
    "    'beta1' : 0.5,\n",
    "    'beta2' : 0.999,\n",
    "    'batch_size' : 32,\n",
    "    'epochs' : 330,\n",
    "    'seed' : 42\n",
    "    }\n",
    "\n",
    "SEED = PARAMS['seed']\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "experiment.log_parameters(PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f185c324",
   "metadata": {
    "papermill": {
     "duration": 0.011917,
     "end_time": "2024-11-16T21:13:25.155362",
     "exception": false,
     "start_time": "2024-11-16T21:13:25.143445",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Initialize the model, using the parameters we defined at previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d06b4842",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T21:13:25.180541Z",
     "iopub.status.busy": "2024-11-16T21:13:25.180167Z",
     "iopub.status.idle": "2024-11-16T21:13:26.131098Z",
     "shell.execute_reply": "2024-11-16T21:13:26.129913Z"
    },
    "papermill": {
     "duration": 0.96602,
     "end_time": "2024-11-16T21:13:26.133178",
     "exception": false,
     "start_time": "2024-11-16T21:13:25.167158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator:\n",
      "Total params: 54541827, Total trainable params: 54541827\n",
      "Discriminator:\n",
      "Total params: 2768705, Total trainable params: 2768705\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Pix2Pix model\n",
    "model = Pix2Pix(\n",
    "    is_train=True,\n",
    "    netD=PARAMS['netD'],\n",
    "    lambda_L1=PARAMS['lambda_L1'],\n",
    "    is_CGAN=PARAMS['is_CGAN'],\n",
    "    use_upsampling=PARAMS['use_upsampling'],\n",
    "    mode=PARAMS['mode'],\n",
    "    c_hid=PARAMS['c_hid'],\n",
    "    n_layers=PARAMS['n_layers'],\n",
    "    lr=PARAMS['lr'],\n",
    "    beta1=PARAMS['beta1'],\n",
    "    beta2=PARAMS['beta2']\n",
    "    )\n",
    "\n",
    "total_params = sum(p.numel() for p in model.gen.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.gen.parameters() if p.requires_grad)\n",
    "print('Generator:')\n",
    "print(f\"Total params: {total_params}, Total trainable params: {total_trainable_params}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.disc.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.disc.parameters() if p.requires_grad)\n",
    "print('Discriminator:')\n",
    "print(f\"Total params: {total_params}, Total trainable params: {total_trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ba2edda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T21:13:26.159265Z",
     "iopub.status.busy": "2024-11-16T21:13:26.158496Z",
     "iopub.status.idle": "2024-11-16T21:13:30.891171Z",
     "shell.execute_reply": "2024-11-16T21:13:30.890365Z"
    },
    "papermill": {
     "duration": 4.748567,
     "end_time": "2024-11-16T21:13:30.894011",
     "exception": false,
     "start_time": "2024-11-16T21:13:26.145444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.to(DEVICE)\n",
    "\n",
    "# Uncomment here if you want to load a checkpoint\n",
    "gen_path = '/kaggle/input/pix2pix/pytorch/sar2rgb/5/pix2pix_gen_180.pth'\n",
    "disc_path = '/kaggle/input/pix2pix/pytorch/sar2rgb/5/pix2pix_disc_180.pth'\n",
    "model.load_model(gen_path=gen_path,disc_path=disc_path)\n",
    "gen_opt_path = '/kaggle/input/pix2pix/pytorch/sar2rgb/5/pix2pix_gen_opt.pth'\n",
    "disc_opt_path = '/kaggle/input/pix2pix/pytorch/sar2rgb/5/pix2pix_disc_opt.pth'\n",
    "model.load_optimizer(gen_opt_path=gen_opt_path, disc_opt_path=disc_opt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55fa3aa",
   "metadata": {
    "papermill": {
     "duration": 0.011946,
     "end_time": "2024-11-16T21:13:30.918521",
     "exception": false,
     "start_time": "2024-11-16T21:13:30.906575",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now, let's move to model to gpu, if you have acces to one (if you don't, that's also OK). \n",
    "\n",
    "Then we call ``torch.compile`` to compile our model. Pytorch will check our model and try to reduce overhead behind scenes. It will boost our performance. \n",
    "\n",
    "The first time you run the cell, it might take some time as pytorch optimizes our model. \n",
    "\n",
    "If you want to learn more about torch compile, please check [Introduction to torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d117ab36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T21:13:30.943904Z",
     "iopub.status.busy": "2024-11-16T21:13:30.943558Z",
     "iopub.status.idle": "2024-11-16T21:13:33.022456Z",
     "shell.execute_reply": "2024-11-16T21:13:33.021419Z"
    },
    "papermill": {
     "duration": 2.094806,
     "end_time": "2024-11-16T21:13:33.025375",
     "exception": false,
     "start_time": "2024-11-16T21:13:30.930569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db206a5",
   "metadata": {
    "papermill": {
     "duration": 0.011837,
     "end_time": "2024-11-16T21:13:33.055043",
     "exception": false,
     "start_time": "2024-11-16T21:13:33.043206",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here, we prepare our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d753a952",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T21:13:33.080935Z",
     "iopub.status.busy": "2024-11-16T21:13:33.080448Z",
     "iopub.status.idle": "2024-11-16T21:14:34.070558Z",
     "shell.execute_reply": "2024-11-16T21:14:34.069540Z"
    },
    "papermill": {
     "duration": 61.016942,
     "end_time": "2024-11-16T21:14:34.084185",
     "exception": false,
     "start_time": "2024-11-16T21:13:33.067243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total image pairs found: 14400\n"
     ]
    }
   ],
   "source": [
    "# Root Path\n",
    "root_dir = '/kaggle/input/sentinel12-image-pairs-segregated-by-terrain/v_2'\n",
    "split_save_path = '/kaggle/working/split.json'\n",
    "# Load the custom dataset\n",
    "train_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.5], std=[0.5]),\n",
    "])\n",
    "\n",
    "dataset = Sentinel(\n",
    "    root_dir=root_dir, \n",
    "    split_type='train', \n",
    "    transform=train_transforms, \n",
    "    split_mode='random',\n",
    "    split_ratio=(0.9,0.0,0.1),\n",
    "    seed=SEED\n",
    "    )\n",
    "dataset.save_split(output_file=split_save_path)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=PARAMS['batch_size'], \n",
    "    shuffle=True, \n",
    "    num_workers=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd7db1e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T21:14:34.110091Z",
     "iopub.status.busy": "2024-11-16T21:14:34.109715Z",
     "iopub.status.idle": "2024-11-17T09:06:52.556367Z",
     "shell.execute_reply": "2024-11-17T09:06:52.555478Z"
    },
    "papermill": {
     "duration": 42738.46361,
     "end_time": "2024-11-17T09:06:52.559916",
     "exception": false,
     "start_time": "2024-11-16T21:14:34.096306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [181/330] - Loss_D: 0.7119, Loss_G: 11.7473\n",
      "Epoch [182/330] - Loss_D: 0.6893, Loss_G: 11.5023\n",
      "Epoch [183/330] - Loss_D: 0.6484, Loss_G: 11.7095\n",
      "Epoch [184/330] - Loss_D: 0.6379, Loss_G: 12.0768\n",
      "Epoch [185/330] - Loss_D: 0.6306, Loss_G: 12.2417\n",
      "Epoch [186/330] - Loss_D: 0.6330, Loss_G: 12.3654\n",
      "Epoch [187/330] - Loss_D: 0.6361, Loss_G: 12.3996\n",
      "Epoch [188/330] - Loss_D: 0.6350, Loss_G: 12.4607\n",
      "Epoch [189/330] - Loss_D: 0.6366, Loss_G: 12.5035\n",
      "Epoch [190/330] - Loss_D: 0.6355, Loss_G: 12.5279\n",
      "Epoch [191/330] - Loss_D: 0.6383, Loss_G: 12.5556\n",
      "Epoch [192/330] - Loss_D: 0.6406, Loss_G: 12.5565\n",
      "Epoch [193/330] - Loss_D: 0.6443, Loss_G: 12.6008\n",
      "Epoch [194/330] - Loss_D: 0.6442, Loss_G: 12.5969\n",
      "Epoch [195/330] - Loss_D: 0.6431, Loss_G: 12.5896\n",
      "Epoch [196/330] - Loss_D: 0.6400, Loss_G: 12.5857\n",
      "Epoch [197/330] - Loss_D: 0.6422, Loss_G: 12.5984\n",
      "Epoch [198/330] - Loss_D: 0.6434, Loss_G: 12.5965\n",
      "Epoch [199/330] - Loss_D: 0.6440, Loss_G: 12.5897\n",
      "Epoch [200/330] - Loss_D: 0.6451, Loss_G: 12.5950\n",
      "Epoch [201/330] - Loss_D: 0.6372, Loss_G: 12.5730\n",
      "Epoch [202/330] - Loss_D: 0.6420, Loss_G: 12.5843\n",
      "Epoch [203/330] - Loss_D: 0.6469, Loss_G: 12.6021\n",
      "Epoch [204/330] - Loss_D: 0.6435, Loss_G: 12.5925\n",
      "Epoch [205/330] - Loss_D: 0.6453, Loss_G: 12.5780\n",
      "Epoch [206/330] - Loss_D: 0.6406, Loss_G: 12.5761\n",
      "Epoch [207/330] - Loss_D: 0.6409, Loss_G: 12.5745\n",
      "Epoch [208/330] - Loss_D: 0.6433, Loss_G: 12.5699\n",
      "Epoch [209/330] - Loss_D: 0.6442, Loss_G: 12.5726\n",
      "Epoch [210/330] - Loss_D: 0.6443, Loss_G: 12.5634\n",
      "Epoch [211/330] - Loss_D: 0.6437, Loss_G: 12.5562\n",
      "Epoch [212/330] - Loss_D: 0.6399, Loss_G: 12.5463\n",
      "Epoch [213/330] - Loss_D: 0.6423, Loss_G: 12.5446\n",
      "Epoch [214/330] - Loss_D: 0.6418, Loss_G: 12.5473\n",
      "Epoch [215/330] - Loss_D: 0.6421, Loss_G: 12.5405\n",
      "Epoch [216/330] - Loss_D: 0.6386, Loss_G: 12.5217\n",
      "Epoch [217/330] - Loss_D: 0.6454, Loss_G: 12.5552\n",
      "Epoch [218/330] - Loss_D: 0.6406, Loss_G: 12.5315\n",
      "Epoch [219/330] - Loss_D: 0.6435, Loss_G: 12.5574\n",
      "Epoch [220/330] - Loss_D: 0.6415, Loss_G: 12.5250\n",
      "Epoch [221/330] - Loss_D: 0.6448, Loss_G: 12.5090\n",
      "Epoch [222/330] - Loss_D: 0.6396, Loss_G: 12.5171\n",
      "Epoch [223/330] - Loss_D: 0.6388, Loss_G: 12.5158\n",
      "Epoch [224/330] - Loss_D: 0.6409, Loss_G: 12.5109\n",
      "Epoch [225/330] - Loss_D: 0.6403, Loss_G: 12.5443\n",
      "Epoch [226/330] - Loss_D: 0.6392, Loss_G: 12.4954\n",
      "Epoch [227/330] - Loss_D: 0.6399, Loss_G: 12.5056\n",
      "Epoch [228/330] - Loss_D: 0.6388, Loss_G: 12.5121\n",
      "Epoch [229/330] - Loss_D: 0.6390, Loss_G: 12.5010\n",
      "Epoch [230/330] - Loss_D: 0.6357, Loss_G: 12.4914\n",
      "Epoch [231/330] - Loss_D: 0.6399, Loss_G: 12.5015\n",
      "Epoch [232/330] - Loss_D: 0.6397, Loss_G: 12.4994\n",
      "Epoch [233/330] - Loss_D: 0.6378, Loss_G: 12.5193\n",
      "Epoch [234/330] - Loss_D: 0.6354, Loss_G: 12.4960\n",
      "Epoch [235/330] - Loss_D: 0.6379, Loss_G: 12.4854\n",
      "Epoch [236/330] - Loss_D: 0.6402, Loss_G: 12.4708\n",
      "Epoch [237/330] - Loss_D: 0.6357, Loss_G: 12.4948\n",
      "Epoch [238/330] - Loss_D: 0.6360, Loss_G: 12.5067\n",
      "Epoch [239/330] - Loss_D: 0.6340, Loss_G: 12.4764\n",
      "Epoch [240/330] - Loss_D: 0.6353, Loss_G: 12.5105\n",
      "Epoch [241/330] - Loss_D: 0.6349, Loss_G: 12.5086\n",
      "Epoch [242/330] - Loss_D: 0.6326, Loss_G: 12.4967\n",
      "Epoch [243/330] - Loss_D: 0.6302, Loss_G: 12.5001\n",
      "Epoch [244/330] - Loss_D: 0.6325, Loss_G: 12.4945\n",
      "Epoch [245/330] - Loss_D: 0.6343, Loss_G: 12.5078\n",
      "Epoch [246/330] - Loss_D: 0.6297, Loss_G: 12.4999\n",
      "Epoch [247/330] - Loss_D: 0.6311, Loss_G: 12.4918\n",
      "Epoch [248/330] - Loss_D: 0.6325, Loss_G: 12.4978\n",
      "Epoch [249/330] - Loss_D: 0.6257, Loss_G: 12.5075\n",
      "Epoch [250/330] - Loss_D: 0.6295, Loss_G: 12.5239\n",
      "Epoch [251/330] - Loss_D: 0.6247, Loss_G: 12.4980\n",
      "Epoch [252/330] - Loss_D: 0.6420, Loss_G: 12.4966\n",
      "Epoch [253/330] - Loss_D: 0.6236, Loss_G: 12.5105\n",
      "Epoch [254/330] - Loss_D: 0.6243, Loss_G: 12.5259\n",
      "Epoch [255/330] - Loss_D: 0.6220, Loss_G: 12.5151\n",
      "Epoch [256/330] - Loss_D: 0.6244, Loss_G: 12.5349\n",
      "Epoch [257/330] - Loss_D: 0.6492, Loss_G: 12.5132\n",
      "Epoch [258/330] - Loss_D: 0.6192, Loss_G: 12.4664\n",
      "Epoch [259/330] - Loss_D: 0.6190, Loss_G: 12.5233\n",
      "Epoch [260/330] - Loss_D: 0.6296, Loss_G: 12.4800\n",
      "Epoch [261/330] - Loss_D: 0.6174, Loss_G: 12.5036\n",
      "Epoch [262/330] - Loss_D: 0.6576, Loss_G: 12.5516\n",
      "Epoch [263/330] - Loss_D: 0.6169, Loss_G: 12.3881\n",
      "Epoch [264/330] - Loss_D: 0.6130, Loss_G: 12.4974\n",
      "Epoch [265/330] - Loss_D: 0.6357, Loss_G: 12.5033\n",
      "Epoch [266/330] - Loss_D: 0.6140, Loss_G: 12.4876\n",
      "Epoch [267/330] - Loss_D: 0.6151, Loss_G: 12.5134\n",
      "Epoch [268/330] - Loss_D: 0.6154, Loss_G: 12.5452\n",
      "Epoch [269/330] - Loss_D: 0.6299, Loss_G: 12.4928\n",
      "Epoch [270/330] - Loss_D: 0.6144, Loss_G: 12.5325\n",
      "Epoch [271/330] - Loss_D: 0.6117, Loss_G: 12.5544\n",
      "Epoch [272/330] - Loss_D: 0.6170, Loss_G: 12.5367\n",
      "Epoch [273/330] - Loss_D: 0.6099, Loss_G: 12.5397\n",
      "Epoch [274/330] - Loss_D: 0.6310, Loss_G: 12.5145\n",
      "Epoch [275/330] - Loss_D: 0.6112, Loss_G: 12.5216\n",
      "Epoch [276/330] - Loss_D: 0.6022, Loss_G: 12.5435\n",
      "Epoch [277/330] - Loss_D: 0.6033, Loss_G: 12.5788\n",
      "Epoch [278/330] - Loss_D: 0.6758, Loss_G: 12.4086\n",
      "Epoch [279/330] - Loss_D: 0.5951, Loss_G: 12.5264\n",
      "Epoch [280/330] - Loss_D: 0.5980, Loss_G: 12.5784\n",
      "Epoch [281/330] - Loss_D: 0.6014, Loss_G: 12.5633\n",
      "Epoch [282/330] - Loss_D: 0.6283, Loss_G: 12.5323\n",
      "Epoch [283/330] - Loss_D: 0.5962, Loss_G: 12.5455\n",
      "Epoch [284/330] - Loss_D: 0.6270, Loss_G: 12.5183\n",
      "Epoch [285/330] - Loss_D: 0.5933, Loss_G: 12.5812\n",
      "Epoch [286/330] - Loss_D: 0.5942, Loss_G: 12.6111\n",
      "Epoch [287/330] - Loss_D: 0.6301, Loss_G: 12.5558\n",
      "Epoch [288/330] - Loss_D: 0.5883, Loss_G: 12.5938\n",
      "Epoch [289/330] - Loss_D: 0.6085, Loss_G: 12.5898\n",
      "Epoch [290/330] - Loss_D: 0.5857, Loss_G: 12.6225\n",
      "Epoch [291/330] - Loss_D: 0.6562, Loss_G: 12.4206\n",
      "Epoch [292/330] - Loss_D: 0.5821, Loss_G: 12.5668\n",
      "Epoch [293/330] - Loss_D: 0.5817, Loss_G: 12.6248\n",
      "Epoch [294/330] - Loss_D: 0.6379, Loss_G: 12.5321\n",
      "Epoch [295/330] - Loss_D: 0.5773, Loss_G: 12.6040\n",
      "Epoch [296/330] - Loss_D: 0.6171, Loss_G: 12.5846\n",
      "Epoch [297/330] - Loss_D: 0.5853, Loss_G: 12.5804\n",
      "Epoch [298/330] - Loss_D: 0.6036, Loss_G: 12.5825\n",
      "Epoch [299/330] - Loss_D: 0.5761, Loss_G: 12.6482\n",
      "Epoch [300/330] - Loss_D: 0.6201, Loss_G: 12.6359\n",
      "Epoch [301/330] - Loss_D: 0.5788, Loss_G: 12.6323\n",
      "Epoch [302/330] - Loss_D: 0.6399, Loss_G: 12.6552\n",
      "Epoch [303/330] - Loss_D: 0.5649, Loss_G: 12.6657\n",
      "Epoch [304/330] - Loss_D: 0.5623, Loss_G: 12.7348\n",
      "Epoch [305/330] - Loss_D: 0.5635, Loss_G: 12.7974\n",
      "Epoch [306/330] - Loss_D: 0.6301, Loss_G: 12.6401\n",
      "Epoch [307/330] - Loss_D: 0.5479, Loss_G: 12.7612\n",
      "Epoch [308/330] - Loss_D: 0.6015, Loss_G: 12.7838\n",
      "Epoch [309/330] - Loss_D: 0.5686, Loss_G: 12.7427\n",
      "Epoch [310/330] - Loss_D: 0.5447, Loss_G: 12.8554\n",
      "Epoch [311/330] - Loss_D: 0.6252, Loss_G: 12.8507\n",
      "Epoch [312/330] - Loss_D: 0.6037, Loss_G: 12.5798\n",
      "Epoch [313/330] - Loss_D: 0.5357, Loss_G: 12.8503\n",
      "Epoch [314/330] - Loss_D: 0.5384, Loss_G: 12.9156\n",
      "Epoch [315/330] - Loss_D: 0.6233, Loss_G: 12.6743\n",
      "Epoch [316/330] - Loss_D: 0.5309, Loss_G: 12.9182\n",
      "Epoch [317/330] - Loss_D: 0.6048, Loss_G: 12.8745\n",
      "Epoch [318/330] - Loss_D: 0.5726, Loss_G: 12.7640\n",
      "Epoch [319/330] - Loss_D: 0.5243, Loss_G: 12.9886\n",
      "Epoch [320/330] - Loss_D: 0.5989, Loss_G: 12.8206\n",
      "Epoch [321/330] - Loss_D: 0.5173, Loss_G: 13.0414\n",
      "Epoch [322/330] - Loss_D: 0.5212, Loss_G: 13.0584\n",
      "Epoch [323/330] - Loss_D: 0.5197, Loss_G: 13.1322\n",
      "Epoch [324/330] - Loss_D: 0.5970, Loss_G: 12.9129\n",
      "Epoch [325/330] - Loss_D: 0.5029, Loss_G: 13.1624\n",
      "Epoch [326/330] - Loss_D: 0.5071, Loss_G: 13.2042\n",
      "Epoch [327/330] - Loss_D: 0.6458, Loss_G: 12.7586\n",
      "Epoch [328/330] - Loss_D: 0.4957, Loss_G: 13.1725\n",
      "Epoch [329/330] - Loss_D: 0.5584, Loss_G: 13.1975\n",
      "Epoch [330/330] - Loss_D: 0.4964, Loss_G: 13.2330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml ExistingExperiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : cooperative_loan_1746\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/yuulind/sar-2-optical/82525ff1312e4019a566c00a209959c1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epoch [150]     : (181, 330)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lossD [150]     : (0.4956719046831131, 0.7118967096010844)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lossG [150]     : (11.502325859069824, 13.232995834350586)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lossG_GAN [150] : (0.7123293133576711, 1.2881485242313808)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lossG_L1 [150]  : (0.1078305493791898, 0.1196735089023908)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size     : 32\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     beta1          : 0.5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     beta2          : 0.999\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     c_hid          : 64\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epochs         : 330\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     is_CGAN        : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lambda_L1      : 100.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr             : 0.0002\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mode           : nearest\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     n_layers       : 3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     netD           : patch\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     seed           : 42\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     use_upsampling : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Please wait for metadata to finish uploading (timeout is 3600 seconds)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Uploading 1 metrics, params and output messages\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = PARAMS['epochs']\n",
    "len_batch = len(dataloader)\n",
    "save_freq = 5\n",
    "base_path = '/kaggle/working/'\n",
    "for epoch in range(181, num_epochs+1):\n",
    "    total_lossD = 0.0\n",
    "    total_lossG = 0.0\n",
    "    total_lossG_GAN = 0.0\n",
    "    total_lossG_L1 = 0.0\n",
    "    model.train()\n",
    "    for real_images, target_images in dataloader:\n",
    "        real_images, target_images = real_images.to(DEVICE), target_images.to(DEVICE)\n",
    "        losses = model.train_step(real_images, target_images)\n",
    "        total_lossD += losses['loss_D']\n",
    "        total_lossG += losses['loss_G']\n",
    "        total_lossG_GAN += losses['loss_G_GAN']\n",
    "        total_lossG_L1 += losses['loss_G_L1']\n",
    "\n",
    "    # Train    \n",
    "    loss_D = total_lossD / len_batch\n",
    "    loss_G = total_lossG / len_batch\n",
    "    loss_G_GAN = total_lossG_GAN / len_batch\n",
    "    loss_G_L1 = total_lossG_L1 / len_batch\n",
    "\n",
    "    # Log the losses\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] - Loss_D: {loss_D:.4f}, Loss_G: {loss_G:.4f}\")\n",
    "    experiment.log_metrics(\n",
    "        {\n",
    "            'epoch': epoch, \n",
    "            'lossD': loss_D,\n",
    "            'lossG' : loss_G,\n",
    "            'lossG_GAN' : loss_G_GAN,\n",
    "            'lossG_L1' : loss_G_L1\n",
    "        }, \n",
    "        step=epoch\n",
    "        )\n",
    "    \n",
    "    if epoch % save_freq == 0:\n",
    "        gen_path = base_path + f'pix2pix_gen_{epoch}.pth'\n",
    "        disc_path = base_path + f'pix2pix_disc_{epoch}.pth'\n",
    "        model.save_model(gen_path=gen_path,disc_path=disc_path)\n",
    "        model.save_optimizer(gen_opt_path='pix2pix_gen_opt.pth', disc_opt_path='pix2pix_disc_opt.pth')\n",
    "        \n",
    "experiment.end()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1201791,
     "sourceId": 2008381,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 157321,
     "modelInstanceId": 134576,
     "sourceId": 168974,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 42848.859471,
   "end_time": "2024-11-17T09:06:55.207486",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-16T21:12:46.348015",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
